{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91e6883",
   "metadata": {},
   "source": [
    "# 1.基本原理\n",
    "\n",
    "## 1.1 发起请求\n",
    "发送Request（包含额外header信息），等待服务器响应\n",
    "## 1.2 获取响应内容\n",
    "服务器正常响应，获得Response（所获取的页面内容：HTML、Json字符串、二进制数据（图片/视频））\n",
    "## 1.3 解析内容\n",
    "HTML：页面解析库、正则表达式\n",
    "Json：Json对象解析\n",
    "二进制数据：进一步处理\n",
    "## 1.4 保存数据\n",
    "文本、数据库、特定格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d03fb8",
   "metadata": {},
   "source": [
    "## 1.5 一个简单的爬虫（来自ChatGPT）\n",
    "爬取网页标题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891e0d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "                                              0.0/62.6 kB ? eta -:--:--\n",
      "     ------                                   10.2/62.6 kB ? eta -:--:--\n",
      "     ------                                   10.2/62.6 kB ? eta -:--:--\n",
      "     ------                                   10.2/62.6 kB ? eta -:--:--\n",
      "     ------                                   10.2/62.6 kB ? eta -:--:--\n",
      "     ------                                   10.2/62.6 kB ? eta -:--:--\n",
      "     ------------                            20.5/62.6 kB 46.8 kB/s eta 0:00:01\n",
      "     ------------                            20.5/62.6 kB 46.8 kB/s eta 0:00:01\n",
      "     -------------------                     30.7/62.6 kB 65.6 kB/s eta 0:00:01\n",
      "     -------------------                     30.7/62.6 kB 65.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 62.6/62.6 kB 128.9 kB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.1.0-cp39-cp39-win_amd64.whl (97 kB)\n",
      "                                              0.0/97.1 kB ? eta -:--:--\n",
      "                                              0.0/97.1 kB ? eta -:--:--\n",
      "                                              0.0/97.1 kB ? eta -:--:--\n",
      "     ------------                             30.7/97.1 kB ? eta -:--:--\n",
      "     ------------                             30.7/97.1 kB ? eta -:--:--\n",
      "     ----------------                       41.0/97.1 kB 393.8 kB/s eta 0:00:01\n",
      "     ----------------                       41.0/97.1 kB 393.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 97.1/97.1 kB 427.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\cc2\\lib\\site-packages (from requests) (3.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "                                              0.0/123.2 kB ? eta -:--:--\n",
      "     ---                                      10.2/123.2 kB ? eta -:--:--\n",
      "     ---------------------                 71.7/123.2 kB 787.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 123.2/123.2 kB 904.1 kB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "                                              0.0/157.0 kB ? eta -:--:--\n",
      "     ----------                               41.0/157.0 kB ? eta -:--:--\n",
      "     ----------                               41.0/157.0 kB ? eta -:--:--\n",
      "     ----------                               41.0/157.0 kB ? eta -:--:--\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------          122.9/157.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  153.6/157.0 kB 203.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 157.0/157.0 kB 208.4 kB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2023.5.7 charset-normalizer-3.1.0 requests-2.31.0 urllib3-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b60fda26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网页标题： 验证码_哔哩哔哩\n",
      "Overed\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 发送HTTP请求获取网页内容\n",
    "url = \"https://www.bilibili.com\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# 解析HTML内容\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# 提取数据\n",
    "# 在这里，你可以使用Beautiful Soup的方法和CSS选择器来定位和提取所需的HTML元素\n",
    "# 这里以提取网页标题为例\n",
    "title_element = soup.select_one(\"title\")\n",
    "if title_element:\n",
    "    title = title_element.text\n",
    "    print(\"网页标题：\", title) \n",
    "    # 数据存储\n",
    "    # 在这里，你可以将提取的数据保存到本地文件或数据库中\n",
    "    # 这里以保存到文本文件为例\n",
    "    with open(\"output1.txt\",\"w\",encoding=\"utf-8\") as file:\n",
    "        file.write(f\"网页标题：{title}\")\n",
    "    print(\"Overed\")\n",
    "else:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd7aa5",
   "metadata": {},
   "source": [
    "## 1.6 爬取新闻标题与链接（来自ChatGPT）\n",
    "有一些问题，比如css标签似乎有误导致无法爬取正确内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69616aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overed\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://s.weibo.com/top/summary\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "#解析HTML内容\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# 提取数据内容\n",
    "hot_elements = soup.select(\"#pl_top_realtimehot > table > tbody > tr > td.td-02 > a\")\n",
    "hot_list = []\n",
    "\n",
    "for hot_element in hot_elements:\n",
    "    title = hot_element.text.strip()\n",
    "    link = hot_element.get(\"href\")\n",
    "    if link.startswith(\"https://\") or link.startswith(\"//\"):\n",
    "        pass\n",
    "    else:\n",
    "        link = \"https://s.weibo.com\" + link\n",
    "    \n",
    "#     if link.startswith(\"https://\"):\n",
    "#         # 链接是绝对路径，直接使用\n",
    "#     else:\n",
    "#         # 链接是相对路径，需要拼接域名\n",
    "#         link = \"https://s.weibo.com\" + link # 这一行需要缩进\n",
    "\n",
    "    hot = {\n",
    "        \"title\": title,\n",
    "        \"link\": link\n",
    "    }\n",
    "    \n",
    "    hot_list.append(hot)\n",
    "\n",
    "# 打印数据\n",
    "for hot in hot_list:\n",
    "    print(\"热搜标题:\", hot[\"title\"])\n",
    "    print(\"链接:\", hot[\"link\"])\n",
    "    print()\n",
    "    \n",
    "# 存储数据\n",
    "with open(\"output2.txt\",\"w\",encoding=\"utf-8\") as file:\n",
    "    for hot in hot_list:\n",
    "        file.write(f\"热搜标题：{hot['title']}\\n\")\n",
    "        file.write(f\"链接：{hot['link']}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "        \n",
    "print(\"Overed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
